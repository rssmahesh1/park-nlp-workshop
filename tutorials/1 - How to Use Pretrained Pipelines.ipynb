{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://nlp.johnsnowlabs.com\"><img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\"/></a>\n",
    "\n",
    "# How to Use Pretrained Pipelines\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will learn how to use one of Spark NLP's pretrained pipelines. \n",
    "\n",
    "In NLP, there is no \"one size, fits all\". In order to get the best performance you will need to gather data, build your own pipelines, and train your own models. So why would you want to use a pretrained pipelines. There are a number of reasons\n",
    "\n",
    "1. You are learning about Spark NLP.\n",
    "  - If you are learning a library, you want to be able to quickly run code to see what development might look like.\n",
    "2. You want to establish a baseline.\n",
    "  - In order to begin experimenting, you need to establish a baseline. You don't want to have to re-invent well understood techniques and best practices. You can use a pretrained pipeline to establish a good baseline, and then measure the improvements you get from tuning for your data and application.\n",
    "3. You are exploring a data set.\n",
    "  - Perhaps your application requires you to work with a large corpus with which you are unfamiliar. Apache Spark is a great tool for exploring large data sets. When you combine it with Spark NLP, you can easily gather some basic understanding of the vocabulary of your text. Additionally, you can get a first pass at seeing which steps you may need to customize. \n",
    "\n",
    "## This notebook\n",
    "\n",
    "1. How to start with Spark NLP\n",
    "2. The `explain_document_ml` pipeline\n",
    "    - How to load pretrained pipelines\n",
    "    - How to use the light pipeline\n",
    "3. The `match_chunks` pipeline\n",
    "    - Phrase chunking\n",
    "\n",
    "## How to start with Spark NLP\n",
    "\n",
    "When you use Spark NLP, you can quickly get started by importing `sparknlp` and calling `sparknlp.start()`. This will return a `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://96f4787f2f1d:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc6806bf7f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our data into spark. For the purpose of exploring these pipelines we will use just small piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'French author who helped pioner the science-fiction genre. Verne wrate about space, air, and underwater travel before navigable aircrast and practical submarines were invented and before any means of space travel had been devised.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'French author who helped pioner the science-fiction genre. ' \\\n",
    "        'Verne wrate about space, air, and underwater travel before ' \\\n",
    "        'navigable aircrast and practical submarines were invented ' \\\n",
    "        'and before any means of space travel had been devised.' \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.createDataFrame([(text,)], ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Spark up and running and our data loaded, we can explore pipelines. We will start with looking at the `explain_document_ml` pipeline.\n",
    "\n",
    "## The `explain_document_ml` pipeline\n",
    "\n",
    "The `explain_document_ml` pipeline is a good choice if you want to try the classic text processing techniques. Let's download this pipeline.\n",
    "\n",
    "### How to load pretrained pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9.4 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "explain_document_ml = PretrainedPipeline(\n",
    "    'explain_document_ml',  # the name of the pipeline\n",
    "    lang='en' # the language of the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, let's look at the stages of this pipeline\n",
    "\n",
    "#### `explain_document_ml` stages\n",
    "\n",
    "1. Document Assembler\n",
    "  - This creates a document from the given text field(s). In an annotator framework, the document is where the text and annotations are stored.\n",
    "2. Sentence Detector\n",
    "  - This sentence detector uses an algorithm based on Kevin Dias' `pragmatic_segmenter` (https://github.com/diasks2/pragmatic_segmenter).\n",
    "3. Tokenizer\n",
    "  - The tokenizer in Spark NLP is more than just a simple regex based splitter. Check the documentation at https://nlp.johnsnowlabs.com/docs/en/annotators#tokenizer\n",
    "4. Lemmatizer\n",
    "  - A lemmatizer tags a token with its \"lemma\". The \"lemma\" is the entry you would find it under in a dictionary. For example, \"cats\" -> \"cat\", \"geese\" -> \"goose\"\n",
    "5. Stemmer\n",
    "  - A stemmer tags tokens with its stem. The stem is a word without any inflection, and possibly with no or few derivational affixes. In English, there are many words that are equivalent to their stem. For example, \"cats\" -> \"cat\", \"geese\" -> \"gees\", \"manager\" -> \"manag\"\n",
    "6. Part-of-Speech Tagger\n",
    "  - A POS tagger tags tokens with a lexical category. Most people are familiar with nouns, verbs, adjectives, adverbs, prepositions, and pronouns. The acronyms are almost always based the Penn Treebank tags (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). Spark NLP uses a perceptron model for this.\n",
    "7. Spell Checker (Norvig)\n",
    "  - This tags words that may be misspelled with a potential correction. This algorithm is based on the algorithm described by Peter Norvig (http://norvig.com/spell-correct.html)\n",
    "  \n",
    "Let's try annotating with this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = explain_document_ml.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is super easy to run this model against the data frame. If you had a large corpus in Spark, you could process it with this pipeline, for storage, downstream consumption, or further exploration in Spark.\n",
    "\n",
    "Now let's see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |    |    |-- sentence_embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentence: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |    |    |-- sentence_embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |    |    |-- sentence_embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- checked: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |    |    |-- sentence_embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- lemma: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |    |    |-- sentence_embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- stem: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |    |    |-- sentence_embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- pos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |    |    |-- sentence_embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotated_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems pretty complicated. As you use more complex aspects of Spark NLP the purpose of the elements of the schema will become clear. If we had a large data data set, we could process everything and store it, but what if we only want to process a small amount of data - like in the current situation. How long does it take to annotate a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448 ms ± 37.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "annotations = annotated_df.first().asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 seconds, on my machine, seems a little long. \n",
    "\n",
    "### How to use the light pipeline\n",
    "\n",
    "Fortunately, Spark NLP has light pipelines that let you process small pieces of text without the overhead of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.7 ms ± 3.48 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "annotations = explain_document_ml.annotate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32.6 ms is more like it! Now that we can process our data more quickly, let's look at what these annotators are actually giving us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['stem', 'checked', 'lemma', 'document', 'pos', 'token', 'sentence'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = explain_document_ml.annotate(text)\n",
    "annotations.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the document first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 French author who helped pioner the science-fiction genre. Verne wrate about space, air, and underwater travel before navigable aircrast and practical submarines were invented and before any means of space travel had been devised.\n"
     ]
    }
   ],
   "source": [
    "for i, document in enumerate(annotations['document']):\n",
    "    print(i, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the original text that we passed in.\n",
    "\n",
    "Let's look at the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 French author who helped pioner the science-fiction genre.\n",
      " 1 Verne wrate about space, air, and underwater travel before navigable aircrast and practical submarines were invented and before any means of space travel had been devised.\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(annotations['sentence']):\n",
    "    print('{:2d} {}'.format(i, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the sentence detector has correctly split the sentences.\n",
    "\n",
    "Now let's look at the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i token               \n",
      "=========================\n",
      " 0 French              \n",
      " 1 author              \n",
      " 2 who                 \n",
      " 3 helped              \n",
      " 4 pioner              \n",
      " 5 the                 \n",
      " 6 science-fiction     \n",
      " 7 genre               \n",
      " 8 .                   \n",
      " 9 Verne               \n",
      "10 wrate               \n",
      "11 about               \n",
      "12 space               \n",
      "13 ,                   \n",
      "14 air                 \n",
      "15 ,                   \n",
      "16 and                 \n",
      "17 underwater          \n",
      "18 travel              \n",
      "19 before              \n",
      "20 navigable           \n",
      "21 aircrast            \n",
      "22 and                 \n",
      "23 practical           \n",
      "24 submarines          \n",
      "25 were                \n",
      "26 invented            \n",
      "27 and                 \n",
      "28 before              \n",
      "29 any                 \n",
      "30 means               \n",
      "31 of                  \n",
      "32 space               \n",
      "33 travel              \n",
      "34 had                 \n",
      "35 been                \n",
      "36 devised             \n",
      "37 .                   \n"
     ]
    }
   ],
   "source": [
    "print('{:>2s} {:20s}'.format('i', 'token'))\n",
    "print('=' * 25)\n",
    "for i, token in enumerate(annotations['token']):\n",
    "    corrected = token != annotations['checked'][i]\n",
    "    print('{:2d} {:20s}'.format(i, token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plainly some spelling errors, let's see if the spell checker caught them. Here we will print the tokens and the spell checker annotations side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i token                checked             \n",
      "=============================================\n",
      " 0 French               French               \n",
      " 1 author               author               \n",
      " 2 who                  who                  \n",
      " 3 helped               helped               \n",
      " 4 pioner               pioneer              <<<<<<<<<<\n",
      " 5 the                  the                  \n",
      " 6 science-fiction      sciencefiction       <<<<<<<<<<\n",
      " 7 genre                genre                \n",
      " 8 .                    .                    \n",
      " 9 Verne                Verne                \n",
      "10 wrate                wrote                <<<<<<<<<<\n",
      "11 about                about                \n",
      "12 space                space                \n",
      "13 ,                    ,                    \n",
      "14 air                  air                  \n",
      "15 ,                    ,                    \n",
      "16 and                  and                  \n",
      "17 underwater           underwater           \n",
      "18 travel               travel               \n",
      "19 before               before               \n",
      "20 navigable            navigable            \n",
      "21 aircrast             aircraft             <<<<<<<<<<\n",
      "22 and                  and                  \n",
      "23 practical            practical            \n",
      "24 submarines           submarines           \n",
      "25 were                 were                 \n",
      "26 invented             invented             \n",
      "27 and                  and                  \n",
      "28 before               before               \n",
      "29 any                  any                  \n",
      "30 means                means                \n",
      "31 of                   of                   \n",
      "32 space                space                \n",
      "33 travel               travel               \n",
      "34 had                  had                  \n",
      "35 been                 been                 \n",
      "36 devised              devised              \n",
      "37 .                    .                    \n"
     ]
    }
   ],
   "source": [
    "print('{:>2s} {:20s} {:20s}'.format('i', 'token', 'checked'))\n",
    "print('=' * 45)\n",
    "for i, token in enumerate(annotations['token']):\n",
    "    checked = annotations['checked'][i]\n",
    "    point = '<'*10 if token != checked else '' \n",
    "    print('{:2d} {:20s} {:20s} {}'.format(i, token, checked, point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has caught all the mistakes. Although \"science-fiction\" was _mis_-corrected to \"sciencefiction\".\n",
    "\n",
    "Now, let's look at the lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i token                checked              lemma               \n",
      "=================================================================\n",
      " 0 French               French               French               \n",
      " 1 author               author               author               \n",
      " 2 who                  who                  who                  \n",
      " 3 helped               helped               help                 <<<<<<<<<<\n",
      " 4 pioner               pioneer              pioneer              \n",
      " 5 the                  the                  the                  \n",
      " 6 science-fiction      sciencefiction       sciencefiction       \n",
      " 7 genre                genre                genre                \n",
      " 8 .                    .                    .                    \n",
      " 9 Verne                Verne                Verne                \n",
      "10 wrate                wrote                write                <<<<<<<<<<\n",
      "11 about                about                about                \n",
      "12 space                space                space                \n",
      "13 ,                    ,                    ,                    \n",
      "14 air                  air                  air                  \n",
      "15 ,                    ,                    ,                    \n",
      "16 and                  and                  and                  \n",
      "17 underwater           underwater           underwater           \n",
      "18 travel               travel               travel               \n",
      "19 before               before               before               \n",
      "20 navigable            navigable            navigable            \n",
      "21 aircrast             aircraft             aircraft             \n",
      "22 and                  and                  and                  \n",
      "23 practical            practical            practical            \n",
      "24 submarines           submarines           submarine            <<<<<<<<<<\n",
      "25 were                 were                 be                   <<<<<<<<<<\n",
      "26 invented             invented             invent               <<<<<<<<<<\n",
      "27 and                  and                  and                  \n",
      "28 before               before               before               \n",
      "29 any                  any                  any                  \n",
      "30 means                means                mean                 <<<<<<<<<<\n",
      "31 of                   of                   of                   \n",
      "32 space                space                space                \n",
      "33 travel               travel               travel               \n",
      "34 had                  had                  have                 <<<<<<<<<<\n",
      "35 been                 been                 be                   <<<<<<<<<<\n",
      "36 devised              devised              devise               <<<<<<<<<<\n",
      "37 .                    .                    .                    \n"
     ]
    }
   ],
   "source": [
    "print('{:>2s} {:20s} {:20s} {:20s}'.format('i', 'token', 'checked', 'lemma'))\n",
    "print('=' * 65)\n",
    "for i, token in enumerate(annotations['token']):\n",
    "    checked = annotations['checked'][i]\n",
    "    lemma = annotations['lemma'][i]\n",
    "    point = '<'*10 if checked != lemma else '' \n",
    "    print('{:2d} {:20s} {:20s} {:20s} {}'.format(i, token, checked, lemma, point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice thta most of the changes to the lemmatization could be expressed as removing common suffixes, like in stemming. However, words like \"had\", and \"wrote\" are not stemmed correctly.\n",
    "\n",
    "Let's look at the stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i token                checked              lemma                stem                \n",
      "=====================================================================================\n",
      " 0 French               French               French               french              \n",
      " 1 author               author               author               author              \n",
      " 2 who                  who                  who                  who                 \n",
      " 3 helped               helped               help                 help                \n",
      " 4 pioner               pioneer              pioneer              pioneer             \n",
      " 5 the                  the                  the                  the                 \n",
      " 6 science-fiction      sciencefiction       sciencefiction       sciencefict         \n",
      " 7 genre                genre                genre                genr                \n",
      " 8 .                    .                    .                    .                   \n",
      " 9 Verne                Verne                Verne                vern                \n",
      "10 wrate                wrote                write                wrote               \n",
      "11 about                about                about                about               \n",
      "12 space                space                space                space               \n",
      "13 ,                    ,                    ,                    ,                   \n",
      "14 air                  air                  air                  air                 \n",
      "15 ,                    ,                    ,                    ,                   \n",
      "16 and                  and                  and                  and                 \n",
      "17 underwater           underwater           underwater           underwat            \n",
      "18 travel               travel               travel               travel              \n",
      "19 before               before               before               befor               \n",
      "20 navigable            navigable            navigable            navig               \n",
      "21 aircrast             aircraft             aircraft             aircraft            \n",
      "22 and                  and                  and                  and                 \n",
      "23 practical            practical            practical            practic             \n",
      "24 submarines           submarines           submarine            submarin            \n",
      "25 were                 were                 be                   were                \n",
      "26 invented             invented             invent               invent              \n",
      "27 and                  and                  and                  and                 \n",
      "28 before               before               before               befor               \n",
      "29 any                  any                  any                  ani                 \n",
      "30 means                means                mean                 mean                \n",
      "31 of                   of                   of                   of                  \n",
      "32 space                space                space                space               \n",
      "33 travel               travel               travel               travel              \n",
      "34 had                  had                  have                 had                 \n",
      "35 been                 been                 be                   been                \n",
      "36 devised              devised              devise               devis               \n",
      "37 .                    .                    .                    .                   \n"
     ]
    }
   ],
   "source": [
    "print('{:>2s} {:20s} {:20s} {:20s} {:20s}'.format('i', 'token', 'checked', 'lemma', 'stem'))\n",
    "print('=' * 85)\n",
    "for i, token in enumerate(annotations['token']):\n",
    "    checked = annotations['checked'][i]\n",
    "    lemma = annotations['lemma'][i]\n",
    "    stem = annotations['stem'][i]\n",
    "    print('{:2d} {:20s} {:20s} {:20s} {:20s}'.format(i, token, checked, lemma, stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a lot of the stems are not real words, e.g. \"devised\" -> \"devis\". That is because a _stem_ is not technically a word. Also, notice that \"wrote\" has not been modified. That is because it has no affixes to remove, so stemming cannot alter it.\n",
    "\n",
    "Now, let's look at the last stage in the pipeline, part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i token                checked              lemma                stem                 pos                 \n",
      "=========================================================================================================\n",
      " 0 French               French               French               french               JJ                  \n",
      " 1 author               author               author               author               NN                  \n",
      " 2 who                  who                  who                  who                  WP                  \n",
      " 3 helped               helped               help                 help                 VBD                 \n",
      " 4 pioner               pioneer              pioneer              pioneer              NN                  \n",
      " 5 the                  the                  the                  the                  DT                  \n",
      " 6 science-fiction      sciencefiction       sciencefiction       sciencefict          NN                  \n",
      " 7 genre                genre                genre                genr                 NN                  \n",
      " 8 .                    .                    .                    .                    .                   \n",
      " 9 Verne                Verne                Verne                vern                 NNP                 \n",
      "10 wrate                wrote                write                wrote                VBD                 \n",
      "11 about                about                about                about                IN                  \n",
      "12 space                space                space                space                NN                  \n",
      "13 ,                    ,                    ,                    ,                    ,                   \n",
      "14 air                  air                  air                  air                  NN                  \n",
      "15 ,                    ,                    ,                    ,                    ,                   \n",
      "16 and                  and                  and                  and                  CC                  \n",
      "17 underwater           underwater           underwater           underwat             JJ                  \n",
      "18 travel               travel               travel               travel               NN                  \n",
      "19 before               before               before               befor                IN                  \n",
      "20 navigable            navigable            navigable            navig                JJ                  \n",
      "21 aircrast             aircraft             aircraft             aircraft             NN                  \n",
      "22 and                  and                  and                  and                  CC                  \n",
      "23 practical            practical            practical            practic              JJ                  \n",
      "24 submarines           submarines           submarine            submarin             NNS                 \n",
      "25 were                 were                 be                   were                 VBD                 \n",
      "26 invented             invented             invent               invent               VBN                 \n",
      "27 and                  and                  and                  and                  CC                  \n",
      "28 before               before               before               befor                IN                  \n",
      "29 any                  any                  any                  ani                  DT                  \n",
      "30 means                means                mean                 mean                 NNS                 \n",
      "31 of                   of                   of                   of                   IN                  \n",
      "32 space                space                space                space                NN                  \n",
      "33 travel               travel               travel               travel               NN                  \n",
      "34 had                  had                  have                 had                  VBD                 \n",
      "35 been                 been                 be                   been                 VBN                 \n",
      "36 devised              devised              devise               devis                VBN                 \n",
      "37 .                    .                    .                    .                    .                   \n"
     ]
    }
   ],
   "source": [
    "print('{:>2s} {:20s} {:20s} {:20s} {:20s} {:20s}'.format('i', 'token', 'checked', 'lemma', 'stem', 'pos'))\n",
    "print('=' * 105)\n",
    "for i, token in enumerate(annotations['token']):\n",
    "    checked = annotations['checked'][i]\n",
    "    lemma = annotations['lemma'][i]\n",
    "    stem = annotations['stem'][i]\n",
    "    pos = annotations['pos'][i]\n",
    "    print('{:2d} {:20s} {:20s} {:20s} {:20s} {:20s}'.format(i, token, checked, lemma, stem, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot of lexical information. In a real situation, as we look at the data we would find issues that we may want to address with custom processing or custom models. But we may also want to combine these tokens into phrases.\n",
    "\n",
    "To do that, we need to look at the next pipeline, `match_chunks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `match_chunks` pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match_chunks pipeline is simpler than the `explain_document_ml` pipeline. It has 4 of the same stages, but it does not have the spell checker, lemmatizer, or stemmer. It does have a _chunker_. \n",
    "\n",
    "### Phrase Chunking\n",
    "\n",
    "In NLP, a _chunker_ is an algorithm that takes tagged tokens and combines them into phrases. In constituency grammars, words, morphemes to be technical, are combined into phrases based on rules controlling how phrase structures can be built. A phrase structure, like a noun phrase, has a head of the same type. So a _noun_ phrase, has a _noun_ head.\n",
    "\n",
    "In Spark NLP, the `chunker` let's you define regular expressions for chunking. Whereas, this will not let you do arbitrary parsing, Human language is much more complicated than that, it will let you quickly and easily create phrase structures you may be interested in.\n",
    "\n",
    "This chunker uses the following regular expression.\n",
    "\n",
    "> `<DT>?<JJ>*<NN>+`\n",
    "\n",
    "This can be read as an optional determiner (\"the\", \"an\", ...), followed by 0 or more adjectives, and 1 or more nouns. In English, this will recognize a large portion of noun phrases.\n",
    "\n",
    "Let's try it out on our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_chunks download started this may take some time.\n",
      "Approx size to download 4.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "match_chunks = PretrainedPipeline('match_chunks', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_text = match_chunks.annotate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's remind ourselves what is in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'French author who helped pioner the science-fiction genre. Verne wrate about space, air, and underwater travel before navigable aircrast and practical submarines were invented and before any means of space travel had been devised.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tokens so we can see why certain things are being marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i token                pos                 \n",
      "=============================================\n",
      " 0 French               JJ                  \n",
      " 1 author               NN                  \n",
      " 2 who                  WP                  \n",
      " 3 helped               VBD                 \n",
      " 4 pioner               NN                  \n",
      " 5 the                  DT                  \n",
      " 6 science-fiction      NN                  \n",
      " 7 genre                NN                  \n",
      " 8 .                    .                   \n",
      " 9 Verne                NNP                 \n",
      "10 wrate                VB                  \n",
      "11 about                IN                  \n",
      "12 space                NN                  \n",
      "13 ,                    ,                   \n",
      "14 air                  NN                  \n",
      "15 ,                    ,                   \n",
      "16 and                  CC                  \n",
      "17 underwater           JJ                  \n",
      "18 travel               NN                  \n",
      "19 before               IN                  \n",
      "20 navigable            JJ                  \n",
      "21 aircrast             NN                  \n",
      "22 and                  CC                  \n",
      "23 practical            JJ                  \n",
      "24 submarines           NNS                 \n",
      "25 were                 VBD                 \n",
      "26 invented             VBN                 \n",
      "27 and                  CC                  \n",
      "28 before               IN                  \n",
      "29 any                  DT                  \n",
      "30 means                NNS                 \n",
      "31 of                   IN                  \n",
      "32 space                NN                  \n",
      "33 travel               NN                  \n",
      "34 had                  VBD                 \n",
      "35 been                 VBN                 \n",
      "36 devised              VBN                 \n",
      "37 .                    .                   \n"
     ]
    }
   ],
   "source": [
    "print('{:>2s} {:20s} {:20s}'.format('i', 'token', 'pos'))\n",
    "print('=' * 45)\n",
    "for i, token in enumerate(chunked_text['token']):\n",
    "    pos = chunked_text['pos'][i]\n",
    "    print('{:2d} {:20s} {:20s}'.format(i, token, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the POS tagger has misidentified \"pioner\" as a noun. Let's look at the chunked noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 French author\n",
      " 1 pioner\n",
      " 2 the science-fiction genre\n",
      " 3 space\n",
      " 4 air\n",
      " 5 underwater travel\n",
      " 6 navigable aircrast\n",
      " 7 space travel\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunked_text['chunk']):\n",
    "    print('{:2d} {}'.format(i, chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has caught most of the noun phrases. It has missed the plural nouns, \"practical submarines\", and the proper nouns, \"Verne\".\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial you learned how to load a pretrained pipeline and run it on Spark data, as well as run them using light pipelines. You can check out more pipelines here (https://nlp.johnsnowlabs.com/docs/en/pipelines).\n",
    "\n",
    "You also learned a little about lemmatizers, stemmers, and phrase chunkers. These techniques can be used with great success to reduce the size of your vocabulary. This especially helps in classification and regression tasks where you might deal with excessively high dimensions and sparsity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
